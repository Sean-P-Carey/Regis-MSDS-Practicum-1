{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "\n",
    "This notebook will take the data that was created in the **Data Collection** Notebook and perform normal text data preprocessing.  \n",
    "\n",
    "**Begin by removing items from the text that are not needed becasue they will add no value to the classification model**\n",
    "\n",
    "* URLs  \n",
    "* hashtags and Twitter @usernames  \n",
    "* Emoticons. \n",
    "* Punctuation  \n",
    "\n",
    "**Next we perform some more common NLP Preprocessing tasks:**\n",
    "\n",
    "* Tokenization\n",
    "* Removal of Stopwords  \n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scarey/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14b311804194935af8e38fea546eb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = \"msds-practicum-carey\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data from AWS S3. \n",
    "\n",
    "To keep the size of the code repository small I have stored all of the data in an S3 Object Store. The other option would be to use GIT LFS. All of the intermediate data has been store as a .pkl (pickle) file. This is a convenient way to serialize any variable from python in a portable way.  \n",
    "\n",
    "tweet_df.pkl is a serialized Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outdata/tweet_df.pkl', 'wb') as data:\n",
    "    s3.Bucket(bucket_name).download_fileobj('tweet_df.pkl', data)\n",
    "    \n",
    "tweet_df = pd.read_pickle('outdata/tweet_df.pkl')\n",
    "\n",
    "os.remove('outdata/tweet_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144834</th>\n",
       "      <td>Two years after the Emancipation Proclamation,...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061475</th>\n",
       "      <td>RT @HelenRosenthal: Insights about how to addr...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81576</th>\n",
       "      <td>I spoke with @deni_kamper of @KNWAnews about C...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072562</th>\n",
       "      <td>UPDATE ON OUR TOWNHALL!!! We have a new locati...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196154</th>\n",
       "      <td>A majority of Americans support background che...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859700</th>\n",
       "      <td>My wife and I raised our four kids in Bozeman,...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253233</th>\n",
       "      <td>RT @GunnelsWarren: Spoiler alert: Jamie Dimon ...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677861</th>\n",
       "      <td>We should open an impeachment inquiry so we ca...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110756</th>\n",
       "      <td>RT @SXMProgress: “I think [healthcare] is the ...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164044</th>\n",
       "      <td>We mourn the loss of two Georgia heroes. My co...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class\n",
       "144834   Two years after the Emancipation Proclamation,...     L\n",
       "1061475  RT @HelenRosenthal: Insights about how to addr...     L\n",
       "81576    I spoke with @deni_kamper of @KNWAnews about C...     C\n",
       "1072562  UPDATE ON OUR TOWNHALL!!! We have a new locati...     L\n",
       "1196154  A majority of Americans support background che...     L\n",
       "859700   My wife and I raised our four kids in Bozeman,...     C\n",
       "1253233  RT @GunnelsWarren: Spoiler alert: Jamie Dimon ...     L\n",
       "677861   We should open an impeachment inquiry so we ca...     L\n",
       "110756   RT @SXMProgress: “I think [healthcare] is the ...     L\n",
       "1164044  We mourn the loss of two Georgia heroes. My co...     C"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stopwords from NLTK and define text cleaning functions. \n",
    "\n",
    "NLTK keeps a library of \"stopwords\". Thesea are words that will typically show up the most in a text but will add very little substance to the analysis. Exampels of STOPWORDS are: \"THE\", \"AN\", \"a\" etc...\n",
    "\n",
    "We can also add words to the list of stopwords. This is done on a project by project basis dependent upon the origin of the text. In our case the corpus came from Twitter so we know a good portion of it will start with \"RT\" which stands for \"retweet\". It adds nothing to the analysis so we will add it to the list of stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords \n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "stopwords.extend(['RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(words):\n",
    "  \n",
    "    \n",
    "    filtered = filter(lambda word: word not in stopwords, words)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "def lemmatize(text, nlp=nlp):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def clean_text(df):\n",
    "   \n",
    "    df[\"clean_tweets\"] = [lemmatize(x) for x in df['clean_tweets'].tolist()]\n",
    "    print('done')\n",
    "    return df\n",
    "\n",
    "# Gets rid of emojis and some oddly formated strings\n",
    "def remove_emoji(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82031e5581b34ccc99712293989da21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10625473a44245c5adfd1232ef742037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ce208b31234ced97d43fada20e482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c0a1da4c21449d88559313c9c82e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7558916bf8d84082b37d7ebe199c4560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974a63e7895c475b93b1444c8679ff48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e1e03fd49e4be4a552ae2a5a353de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1387a512b4934316b1e1e70830ada29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b0e5af5ccf491b8fefdbe8f344a9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccdda94a9f449c68557e72626c85321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bf784b777044a7876272fa66c6f113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350306.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['tweet'].progress_apply(lambda x: re.sub('http://\\S+', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub('https://\\S+', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub('@\\S+', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: remove_emoji(x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub('\\n', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub('&amp;', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub('&amp', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x: tokenize(x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x : remove_stopwords(x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].progress_apply(lambda x : \" \".join(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 16min 33.9sdone\n",
      "[###                                     ] | 8% Completed | 16min 45.9sdone\n",
      "[######                                  ] | 16% Completed | 16min 56.9sdone\n",
      "[##########                              ] | 25% Completed | 17min  7.7sdone\n",
      "[#############                           ] | 33% Completed | 17min 15.9sdone\n",
      "[################                        ] | 41% Completed | 17min 25.3sdone\n",
      "[####################                    ] | 50% Completed | 17min 34.7sdone\n",
      "[#######################                 ] | 58% Completed | 17min 41.4sdone\n",
      "[##########################              ] | 66% Completed | 17min 46.9sdone\n",
      "[##############################          ] | 75% Completed | 17min 52.4sdone\n",
      "[#################################       ] | 83% Completed | 17min 56.6sdone\n",
      "[####################################    ] | 91% Completed | 18min  0.1sdone\n",
      "[########################################] | 100% Completed | 18min  1.4s\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "dask_df = ddf.from_pandas(tweet_df, npartitions = parts)\n",
    "result = dask_df.map_partitions(clean_text, meta = tweet_df)\n",
    "with ProgressBar():\n",
    "    df = result.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @aafb: Congrats to ⁦@RepOHalleran⁩ &amp;amp; ⁦@...</td>\n",
       "      <td>L</td>\n",
       "      <td>congrat appointment look forward work together h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great to meet the new Lake County Farm Bureau ...</td>\n",
       "      <td>L</td>\n",
       "      <td>great meet new lake county farm bureau executi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations to @waynestcollege women's rug...</td>\n",
       "      <td>C</td>\n",
       "      <td>congratulation women rugby win sixth national ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great to meet with the Erickson Air Crane team...</td>\n",
       "      <td>C</td>\n",
       "      <td>great meet erickson air crane team medford tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Always wonderful to be part of the Back to Sch...</td>\n",
       "      <td>L</td>\n",
       "      <td>always wonderful part back school jam resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350301</th>\n",
       "      <td>We should be upholding the National Environmen...</td>\n",
       "      <td>L</td>\n",
       "      <td>-PRON- uphold national environmental policy ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350302</th>\n",
       "      <td>If anything is to be investigated, I think we ...</td>\n",
       "      <td>C</td>\n",
       "      <td>if anything investigate -PRON- think need inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350303</th>\n",
       "      <td>TODAY: Federal judge rules in favor of House R...</td>\n",
       "      <td>C</td>\n",
       "      <td>today federal judge rule favor house republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350304</th>\n",
       "      <td>In the words of an old proverb, \"A hit dog wil...</td>\n",
       "      <td>L</td>\n",
       "      <td>in word old proverb a hit dog holler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350305</th>\n",
       "      <td>The new EPA regs are pure fantasy. http://t.co...</td>\n",
       "      <td>C</td>\n",
       "      <td>the new epa reg pure fantasy utpol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350306 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class  \\\n",
       "0        RT @aafb: Congrats to ⁦@RepOHalleran⁩ &amp; ⁦@...     L   \n",
       "1        Great to meet the new Lake County Farm Bureau ...     L   \n",
       "2        Congratulations to @waynestcollege women's rug...     C   \n",
       "3        Great to meet with the Erickson Air Crane team...     C   \n",
       "4        Always wonderful to be part of the Back to Sch...     L   \n",
       "...                                                    ...   ...   \n",
       "1350301  We should be upholding the National Environmen...     L   \n",
       "1350302  If anything is to be investigated, I think we ...     C   \n",
       "1350303  TODAY: Federal judge rules in favor of House R...     C   \n",
       "1350304  In the words of an old proverb, \"A hit dog wil...     L   \n",
       "1350305  The new EPA regs are pure fantasy. http://t.co...     C   \n",
       "\n",
       "                                              clean_tweets  \n",
       "0         congrat appointment look forward work together h  \n",
       "1        great meet new lake county farm bureau executi...  \n",
       "2        congratulation women rugby win sixth national ...  \n",
       "3        great meet erickson air crane team medford tod...  \n",
       "4        always wonderful part back school jam resource...  \n",
       "...                                                    ...  \n",
       "1350301  -PRON- uphold national environmental policy ac...  \n",
       "1350302  if anything investigate -PRON- think need inve...  \n",
       "1350303  today federal judge rule favor house republica...  \n",
       "1350304               in word old proverb a hit dog holler  \n",
       "1350305                 the new epa reg pure fantasy utpol  \n",
       "\n",
       "[1350306 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outdata/tweets_clean_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    \n",
    "s3.meta.client.upload_file('outdata/tweets_clean_df.pkl',\n",
    "                           bucket_name,\n",
    "                           'tweets_clean_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-207239:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: unpickling stack underflow\n",
      "Process ForkPoolWorker-354237:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/scarey/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: invalid load key, '\\xf1'.\n"
     ]
    }
   ],
   "source": [
    "os.remove('outdata/tweets_clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
