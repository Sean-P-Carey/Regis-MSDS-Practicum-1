{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "\n",
    "This notebook will take the data that was created in the **Data Collection** Notebook and perform normal text data preprocessing.  \n",
    "\n",
    "**Begin by removing items from the text that are not needed becasue they will add no value to the classification model**\n",
    "\n",
    "* URLs  \n",
    "* hashtags and Twitter @usernames  \n",
    "* Emoticons. \n",
    "* Punctuation  \n",
    "\n",
    "**Next we perform some more common NLP Preprocessing tasks:**\n",
    "\n",
    "* Tokenization\n",
    "* Removal of Stopwords  \n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = \"msds-practicum-carey\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data from AWS S3. \n",
    "\n",
    "To keep the size of the code repository small I have stored all of the data in an S3 Object Store. The other option would be to use GIT LFS. All of the intermediate data has been store as a .pkl (pickle) file. This is a convenient way to serialize any variable from python in a portable way.  \n",
    "\n",
    "tweet_df.pkl is a serialized Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outdata/tweet_df.pkl', 'wb') as data:\n",
    "    s3.Bucket(bucket_name).download_fileobj('tweet_df.pkl', data)\n",
    "    \n",
    "tweet_df = pd.read_pickle('outdata/tweet_df.pkl')\n",
    "\n",
    "os.remove('outdata/tweet_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1056834</th>\n",
       "      <td>Homeland Security Committee hearing on #TSA &amp;a...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613094</th>\n",
       "      <td>The last major US nail manufacturer--and their...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405118</th>\n",
       "      <td>In case you missed it, @SpeakerRyan, @RepKevin...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224362</th>\n",
       "      <td>VP Biden is on his last stop of the day in Ohi...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643432</th>\n",
       "      <td>I fundamentally refuse to let Americans pay mo...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842083</th>\n",
       "      <td>RT @JasonKander: When I was Secretary of State...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213864</th>\n",
       "      <td>RT @cppj: Stay up-to-date on state road and hi...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500238</th>\n",
       "      <td>RT @NJTVNews: .@SenatorMenendez calls for $2.5...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599936</th>\n",
       "      <td>RT @DonDaileyAPT: Tonight @ 8 on @CapitolJourn...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284711</th>\n",
       "      <td>RT @HouseJudiciary: Statement from @GOPLeader,...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class\n",
       "1056834  Homeland Security Committee hearing on #TSA &a...     L\n",
       "613094   The last major US nail manufacturer--and their...     L\n",
       "405118   In case you missed it, @SpeakerRyan, @RepKevin...     C\n",
       "1224362  VP Biden is on his last stop of the day in Ohi...     L\n",
       "643432   I fundamentally refuse to let Americans pay mo...     L\n",
       "842083   RT @JasonKander: When I was Secretary of State...     L\n",
       "1213864  RT @cppj: Stay up-to-date on state road and hi...     C\n",
       "500238   RT @NJTVNews: .@SenatorMenendez calls for $2.5...     L\n",
       "599936   RT @DonDaileyAPT: Tonight @ 8 on @CapitolJourn...     C\n",
       "284711   RT @HouseJudiciary: Statement from @GOPLeader,...     C"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stopwords from NLTK and define text cleaning functions. \n",
    "\n",
    "NLTK keeps a library of \"stopwords\". Thesea are words that will typically show up the most in a text but will add very little substance to the analysis. Exampels of STOPWORDS are: \"THE\", \"AN\", \"a\" etc...\n",
    "\n",
    "We can also add words to the list of stopwords. This is done on a project by project basis dependent upon the origin of the text. In our case the corpus came from Twitter so we know a good portion of it will start with \"RT\" which stands for \"retweet\". It adds nothing to the analysis so we will add it to the list of stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords \n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "stopwords.extend(['RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# breaks text up in to a list of individual words \n",
    "def tokenize(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# removes stopwords \n",
    "def remove_stopwords(words):\n",
    "  \n",
    "    \n",
    "    filtered = filter(lambda word: word not in stopwords, words)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "#  lemmatizes text based on the part of speech tags \n",
    "def lemmatize(text, nlp=nlp):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# applies the lemmatize function to a dataframe\n",
    "# allows us to use Dask to run function in parallel\n",
    "def clean_text(df):\n",
    "   \n",
    "    df[\"clean_tweets\"] = [lemmatize(x) for x in df['clean_tweets'].tolist()]\n",
    "    print('done')\n",
    "    return df\n",
    "\n",
    "# Gets rid of emojis and some oddly formated strings\n",
    "def remove_emoji(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use REGEX and the defined functions to perform  preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['tweet'].apply(lambda x: re.sub('http://\\S+','', x))\n",
    "\n",
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub('https://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove @name mentions and Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub('@\\S+', '', x))\n",
    "\n",
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove new line Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub('\\n', '', x))\n",
    "\n",
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove amperstand (&) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub('&amp;', '', x))\n",
    "\n",
    "tweet_df['clean_tweets'] =\\\n",
    "tweet_df['clean_tweets'].apply(lambda x: re.sub('&amp', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tokenize, Remove Stopwords, rejoint into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: tokenize(x))\n",
    "\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x :\\\n",
    "                                                          remove_stopwords(x))\n",
    "\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask to parallelize the lemmatization of the words. \n",
    "\n",
    "The goal of lemmatization is to remove the inflection from the words. Returning only the base word.  \n",
    "\n",
    "Processing each of the 1.3 million tweets one at a time will take a long time becasue lemmatizing a sentence is computationally expensive. To speed up this process we will use the \"Dask\" package.  \n",
    "\n",
    "Using Dask we can break the dataframe up in to separate partitions and have each of them processed by a separate core of the processor. This is known as parallel computing. \n",
    "\n",
    "We begin by getting the number of cores within the computers processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parts = os.cpu_count()\n",
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use Dask to break the Pandas Dataframe up in to the same number of paritions as we have cores. Then we map the 'clean_text' function to each parition and process.  \n",
    "\n",
    "On my machine a 60 minute operation was reduced to 15 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 16min 52.9sdone\n",
      "[###                                     ] | 8% Completed | 17min  1.8sdone\n",
      "[######                                  ] | 16% Completed | 17min 10.5sdone\n",
      "[##########                              ] | 25% Completed | 17min 18.2sdone\n",
      "[#############                           ] | 33% Completed | 17min 24.2sdone\n",
      "[################                        ] | 41% Completed | 17min 27.4sdone\n",
      "[####################                    ] | 50% Completed | 17min 32.4sdone\n",
      "[#######################                 ] | 58% Completed | 17min 36.4sdone\n",
      "[#######################                 ] | 58% Completed | 17min 38.2sdone\n",
      "[##############################          ] | 75% Completed | 17min 41.0sdone\n",
      "[##############################          ] | 75% Completed | 17min 42.3sdone\n",
      "[##############################          ] | 75% Completed | 17min 42.8sdone\n",
      "[########################################] | 100% Completed | 17min 44.1s\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "dask_df = ddf.from_pandas(tweet_df, npartitions = parts)\n",
    "result = dask_df.map_partitions(clean_text, meta = tweet_df)\n",
    "with ProgressBar():\n",
    "    df = result.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a new dataframe that contains all of the original data plus a new column that contains the lemmatized thext.  \n",
    "\n",
    "Lemmatizing the text will make it easier to get correct word counts and such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1099733</th>\n",
       "      <td>Great to chat with some of my #TX22 bosses, th...</td>\n",
       "      <td>C</td>\n",
       "      <td>great chat tx22 boss robison family town sprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31218</th>\n",
       "      <td>Staff participated in National Service Day pro...</td>\n",
       "      <td>L</td>\n",
       "      <td>staff participate national service day program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368405</th>\n",
       "      <td>When I was at #ParamountHighSchool’s Senior Aw...</td>\n",
       "      <td>L</td>\n",
       "      <td>when -PRON- paramounthighschool senior awards ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304583</th>\n",
       "      <td>Wisconsin has lagged in business start-up acti...</td>\n",
       "      <td>L</td>\n",
       "      <td>wisconsin lag business startup activity -PRON-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484127</th>\n",
       "      <td>Enjoyed learning about the @wvuLibraries archi...</td>\n",
       "      <td>C</td>\n",
       "      <td>enjoy learn archive process even get chance ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954665</th>\n",
       "      <td>RT @neilwymt: The #SOARSummit is wrapping up, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>the soarsummit wrapping coverage continue spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316896</th>\n",
       "      <td>RT @CCSTorg: This morning @RepGaramendi met wi...</td>\n",
       "      <td>L</td>\n",
       "      <td>this morning meet alumnus share pride help cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952198</th>\n",
       "      <td>RT @DarrellIssa: RT @GOPoversight: Contempt re...</td>\n",
       "      <td>C</td>\n",
       "      <td>contempt resolution vote tally 255 yea 67 nay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185124</th>\n",
       "      <td>Interesting and personal story about our SOS n...</td>\n",
       "      <td>C</td>\n",
       "      <td>interesting personal story sos nominee everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506776</th>\n",
       "      <td>Adam Jobbers-Miller was a patriot, dedicated t...</td>\n",
       "      <td>L</td>\n",
       "      <td>adam jobbersmiller patriot dedicated community...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141123</th>\n",
       "      <td>.@realDonaldTrump needs to realize: No one is ...</td>\n",
       "      <td>L</td>\n",
       "      <td>need realize no one right call question legiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325166</th>\n",
       "      <td>American workers don’t need NAFTA with a new n...</td>\n",
       "      <td>L</td>\n",
       "      <td>american worker do not need nafta new name the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224738</th>\n",
       "      <td>I’m calling on @realDonaldTrump to sign this i...</td>\n",
       "      <td>C</td>\n",
       "      <td>-PRON- be call sign important legislation prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148018</th>\n",
       "      <td>Trump Org to Congress: the Constitution degrad...</td>\n",
       "      <td>L</td>\n",
       "      <td>trump org congress constitution degrade custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907392</th>\n",
       "      <td>Want to join Team Moulton? Now accepting appli...</td>\n",
       "      <td>L</td>\n",
       "      <td>want join team moulton now accept application ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30959</th>\n",
       "      <td>5 years ago I watched Pres. Obama sign the Aff...</td>\n",
       "      <td>L</td>\n",
       "      <td>5 year ago -PRON- watch pre obama sign afforda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968455</th>\n",
       "      <td>Too many students today attend school in crumb...</td>\n",
       "      <td>L</td>\n",
       "      <td>too many student today attend school crumble b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274732</th>\n",
       "      <td>RT @mike_pence: Thanks to today's vote in Cong...</td>\n",
       "      <td>C</td>\n",
       "      <td>thank todays vote congress one step close repe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221963</th>\n",
       "      <td>For over 230 years, the U.S. Constitution has ...</td>\n",
       "      <td>C</td>\n",
       "      <td>for 230 year us constitution promote value ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283486</th>\n",
       "      <td>RT @Jim_Jordan: This isn’t impeachment. This i...</td>\n",
       "      <td>C</td>\n",
       "      <td>this be not impeachment this political campaig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class  \\\n",
       "1099733  Great to chat with some of my #TX22 bosses, th...     C   \n",
       "31218    Staff participated in National Service Day pro...     L   \n",
       "368405   When I was at #ParamountHighSchool’s Senior Aw...     L   \n",
       "304583   Wisconsin has lagged in business start-up acti...     L   \n",
       "484127   Enjoyed learning about the @wvuLibraries archi...     C   \n",
       "954665   RT @neilwymt: The #SOARSummit is wrapping up, ...     C   \n",
       "1316896  RT @CCSTorg: This morning @RepGaramendi met wi...     L   \n",
       "952198   RT @DarrellIssa: RT @GOPoversight: Contempt re...     C   \n",
       "1185124  Interesting and personal story about our SOS n...     C   \n",
       "506776   Adam Jobbers-Miller was a patriot, dedicated t...     L   \n",
       "141123   .@realDonaldTrump needs to realize: No one is ...     L   \n",
       "325166   American workers don’t need NAFTA with a new n...     L   \n",
       "1224738  I’m calling on @realDonaldTrump to sign this i...     C   \n",
       "1148018  Trump Org to Congress: the Constitution degrad...     L   \n",
       "907392   Want to join Team Moulton? Now accepting appli...     L   \n",
       "30959    5 years ago I watched Pres. Obama sign the Aff...     L   \n",
       "968455   Too many students today attend school in crumb...     L   \n",
       "274732   RT @mike_pence: Thanks to today's vote in Cong...     C   \n",
       "1221963  For over 230 years, the U.S. Constitution has ...     C   \n",
       "1283486  RT @Jim_Jordan: This isn’t impeachment. This i...     C   \n",
       "\n",
       "                                              clean_tweets  \n",
       "1099733  great chat tx22 boss robison family town sprin...  \n",
       "31218    staff participate national service day program...  \n",
       "368405   when -PRON- paramounthighschool senior awards ...  \n",
       "304583   wisconsin lag business startup activity -PRON-...  \n",
       "484127   enjoy learn archive process even get chance ch...  \n",
       "954665   the soarsummit wrapping coverage continue spec...  \n",
       "1316896  this morning meet alumnus share pride help cre...  \n",
       "952198   contempt resolution vote tally 255 yea 67 nay ...  \n",
       "1185124  interesting personal story sos nominee everyon...  \n",
       "506776   adam jobbersmiller patriot dedicated community...  \n",
       "141123   need realize no one right call question legiti...  \n",
       "325166   american worker do not need nafta new name the...  \n",
       "1224738  -PRON- be call sign important legislation prom...  \n",
       "1148018  trump org congress constitution degrade custom...  \n",
       "907392   want join team moulton now accept application ...  \n",
       "30959    5 year ago -PRON- watch pre obama sign afforda...  \n",
       "968455   too many student today attend school crumble b...  \n",
       "274732   thank todays vote congress one step close repe...  \n",
       "1221963  for 230 year us constitution promote value ind...  \n",
       "1283486  this be not impeachment this political campaig...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('outdata/tweets_clean_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    \n",
    "s3.meta.client.upload_file('outdata/tweets_clean_df.pkl',\n",
    "                           bucket_name,\n",
    "                           'tweets_clean_df.pkl')\n",
    "\n",
    "os.remove('outdata/tweets_clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
