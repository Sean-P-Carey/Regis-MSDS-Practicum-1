{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "\n",
    "This notebook will take the data that was created in the **Data Collection** Notebook and perform normal text data preprocessing.  \n",
    "\n",
    "**Begin by removing items from the text that are not needed becasue they will add no value to the classification model**\n",
    "\n",
    "* URLs  \n",
    "* hashtags and Twitter @usernames  \n",
    "* Emoticons. \n",
    "* Punctuation  \n",
    "\n",
    "**Next we perform some more common NLP Preprocessing tasks:**\n",
    "\n",
    "* Tokenization\n",
    "* Removal of Stopwords  \n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = \"msds-practicum-carey\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data from AWS S3. \n",
    "\n",
    "To keep the size of the code repository small I have stored all of the data in an S3 Object Store. The other option would be to use GIT LFS. All of the intermediate data has been store as a .pkl (pickle) file. This is a convenient way to serialize any variable from python in a portable way.  \n",
    "\n",
    "tweet_df.pkl is a serialized Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outdata/tweet_df.pkl', 'wb') as data:\n",
    "    s3.Bucket(bucket_name).download_fileobj('tweet_df.pkl', data)\n",
    "    \n",
    "tweet_df = pd.read_pickle('outdata/tweet_df.pkl')\n",
    "\n",
    "os.remove('outdata/tweet_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688755</th>\n",
       "      <td>NBC News reports Obama knew for at least 3 yea...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114683</th>\n",
       "      <td>Thank you Naomi and family for visiting and sh...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660825</th>\n",
       "      <td>Bernie Sanders and I and all the members of th...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757280</th>\n",
       "      <td>RT @SpeakerPelosi: The House will vote on Thur...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199501</th>\n",
       "      <td>RI manufacturers employ over 42,000 workers &amp;a...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025881</th>\n",
       "      <td>Just visited Mason Valley's Peri &amp;amp; Sons Fa...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179315</th>\n",
       "      <td>Great discussion about the future of housing i...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429852</th>\n",
       "      <td>When Trump ended DACA, he left hundreds of tho...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349528</th>\n",
       "      <td>RT @Ryan_ILFB: Great to hear from @RodneyDavis...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958124</th>\n",
       "      <td>Thanks to the folks in the Air Traffic Control...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class\n",
       "688755   NBC News reports Obama knew for at least 3 yea...     C\n",
       "1114683  Thank you Naomi and family for visiting and sh...     L\n",
       "660825   Bernie Sanders and I and all the members of th...     C\n",
       "757280   RT @SpeakerPelosi: The House will vote on Thur...     L\n",
       "1199501  RI manufacturers employ over 42,000 workers &a...     L\n",
       "1025881  Just visited Mason Valley's Peri &amp; Sons Fa...     L\n",
       "179315   Great discussion about the future of housing i...     C\n",
       "429852   When Trump ended DACA, he left hundreds of tho...     L\n",
       "1349528  RT @Ryan_ILFB: Great to hear from @RodneyDavis...     C\n",
       "958124   Thanks to the folks in the Air Traffic Control...     C"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stopwords from NLTK and define text cleaning functions. \n",
    "\n",
    "NLTK keeps a library of \"stopwords\". Thesea are words that will typically show up the most in a text but will add very little substance to the analysis. Exampels of STOPWORDS are: \"THE\", \"AN\", \"a\" etc...\n",
    "\n",
    "We can also add words to the list of stopwords. This is done on a project by project basis dependent upon the origin of the text. In our case the corpus came from Twitter so we know a good portion of it will start with \"RT\" which stands for \"retweet\". It adds nothing to the analysis so we will add it to the list of stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords \n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "stopwords.extend(['RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# breaks text up in to a list of individual words \n",
    "def tokenize(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# removes stopwords \n",
    "def remove_stopwords(words):\n",
    "  \n",
    "    \n",
    "    filtered = filter(lambda word: word not in stopwords, words)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "#  lemmatizes text based on the part of speech tags \n",
    "def lemmatize(text, nlp=nlp):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# applies the lemmatize function to a dataframe\n",
    "# allows us to use Dask to run function in parallel\n",
    "def clean_text(df):\n",
    "   \n",
    "    df[\"clean_tweets\"] = [lemmatize(x) for x in df['clean_tweets'].tolist()]\n",
    "    print('done')\n",
    "    return df\n",
    "\n",
    "# Gets rid of emojis and some oddly formated strings\n",
    "def remove_emoji(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use REGEX and the defined functions to perform  preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['tweet'].apply(lambda x: re.sub('http://\\S+', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub('https://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove @name mentions and Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub('@\\S+', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove new line Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub('\\n', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove amperstand (&) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub('&amp;', '', x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: re.sub('&amp', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tokenize, Remove Stopwords, rejoint into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x: tokenize(x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x : remove_stopwords(x))\n",
    "tweet_df['clean_tweets'] = tweet_df['clean_tweets'].apply(lambda x : \" \".join(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask to parallelize the lemmatization of the words. \n",
    "\n",
    "The goal of lemmatization is to remove the inflection from the words. Returning only the base word.  \n",
    "\n",
    "Processing each of the 1.3 million tweets one at a time will take a long time becasue lemmatizing a sentence is computationally expensive. To speed up this process we will use the \"Dask\" package.  \n",
    "\n",
    "Using Dask we can break the dataframe up in to separate partitions and have each of them processed by a separate core of the processor. This is known as parallel computing. \n",
    "\n",
    "We begin by getting the number of cores within the computers processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parts = os.cpu_count()\n",
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use Dask to break the Pandas Dataframe up in to the same number of paritions as we have cores. Then we map the 'clean_text' function to each parition and process.  \n",
    "\n",
    "On my machine a 60 minute operation was reduced to 15 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scarey/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 14min 41.8sdone\n",
      "[###                                     ] | 8% Completed | 14min 49.5sdone\n",
      "[######                                  ] | 16% Completed | 14min 55.8sdone\n",
      "[##########                              ] | 25% Completed | 15min  4.0sdone\n",
      "[#############                           ] | 33% Completed | 15min  9.1sdone\n",
      "[################                        ] | 41% Completed | 15min 13.5sdone\n",
      "[####################                    ] | 50% Completed | 15min 17.5sdone\n",
      "[#######################                 ] | 58% Completed | 15min 20.6sdone\n",
      "[##########################              ] | 66% Completed | 15min 23.6sdone\n",
      "[##############################          ] | 75% Completed | 15min 26.2sdone\n",
      "[##############################          ] | 75% Completed | 15min 27.5sdone\n",
      "[####################################    ] | 91% Completed | 15min 29.3sdone\n",
      "[########################################] | 100% Completed | 15min 30.5s\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "dask_df = ddf.from_pandas(tweet_df, npartitions = parts)\n",
    "result = dask_df.map_partitions(clean_text, meta = tweet_df)\n",
    "with ProgressBar():\n",
    "    df = result.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a new dataframe that contains all of the original data plus a new column that contains the lemmatized thext.  \n",
    "\n",
    "Lemmatizing the text will make it easier to get correct word counts and such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1098648</th>\n",
       "      <td>.@POTUS doesn't know what it's like to live pa...</td>\n",
       "      <td>L</td>\n",
       "      <td>do not know like live paycheck paycheck hell n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460691</th>\n",
       "      <td>RT @SDAgriculture: Thank you @SDGovDaugaard fo...</td>\n",
       "      <td>C</td>\n",
       "      <td>thank declare yesterday e15 day south dakota -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844170</th>\n",
       "      <td>ICYMI: Always enjoy mornings with @cspan @cspa...</td>\n",
       "      <td>C</td>\n",
       "      <td>icymi always enjoy morning thank cspanwj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536961</th>\n",
       "      <td>Kathleen's Women- &amp;amp; Minority-Owned Busines...</td>\n",
       "      <td>L</td>\n",
       "      <td>kathleens women minorityowned business resourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901461</th>\n",
       "      <td>Members of @VETSports discuss efforts to assis...</td>\n",
       "      <td>L</td>\n",
       "      <td>member discuss effort assist veteran veteransd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189330</th>\n",
       "      <td>Bad #trade deals have resulted in lost #jobs a...</td>\n",
       "      <td>L</td>\n",
       "      <td>bad trade deal result lose job shuttered facto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333424</th>\n",
       "      <td>The Ag portion of the minibus really focuses o...</td>\n",
       "      <td>C</td>\n",
       "      <td>the ag portion minibus really focus ruralameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693819</th>\n",
       "      <td>What's in the bill? Critical support for:\\n\\n•...</td>\n",
       "      <td>L</td>\n",
       "      <td>what s bill critical support forpuerto ricos r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605614</th>\n",
       "      <td>Looking forward to joining @CNNSOTU on Sunday ...</td>\n",
       "      <td>L</td>\n",
       "      <td>look forward join sunday morning tune 8 a.m. cst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217669</th>\n",
       "      <td>#Bismarck Century students Ronak, Bryce, Erik ...</td>\n",
       "      <td>C</td>\n",
       "      <td>bismarck century student ronak bryce erik doug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222200</th>\n",
       "      <td>Today is #TaxDay, which is a good reminder tha...</td>\n",
       "      <td>L</td>\n",
       "      <td>today taxday good reminder goptaxscam ultimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302774</th>\n",
       "      <td>Read more here: https://t.co/M4qq2JBpGi</td>\n",
       "      <td>C</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252904</th>\n",
       "      <td>Thank you to @Sandridge172 in #Lywood for the ...</td>\n",
       "      <td>L</td>\n",
       "      <td>thank lywood warm welcome yesterday -PRON- ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745510</th>\n",
       "      <td>Larry Willis - President, Transportation Trade...</td>\n",
       "      <td>L</td>\n",
       "      <td>larry willis president transportation trades d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443974</th>\n",
       "      <td>This is not a trifling matter. I urge my colle...</td>\n",
       "      <td>C</td>\n",
       "      <td>this trifle matter -PRON- urge colleague congr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263565</th>\n",
       "      <td>RT @LasVegas_NV_USA: Titus looking out for ani...</td>\n",
       "      <td>L</td>\n",
       "      <td>titus look animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626827</th>\n",
       "      <td>RT @75arcounties: Thank you @RepFrenchHill for...</td>\n",
       "      <td>C</td>\n",
       "      <td>thank speak county judge morning spring meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809495</th>\n",
       "      <td>This is yet another shameful example of how @B...</td>\n",
       "      <td>L</td>\n",
       "      <td>this yet another shameful example care way mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291929</th>\n",
       "      <td>Joined my fellow Minnesotans in Washington, D....</td>\n",
       "      <td>C</td>\n",
       "      <td>joined fellow minnesotans washington dc evenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180205</th>\n",
       "      <td>.@txlegion plays such an important role in ser...</td>\n",
       "      <td>C</td>\n",
       "      <td>play important role serve community teaching k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet class  \\\n",
       "1098648  .@POTUS doesn't know what it's like to live pa...     L   \n",
       "460691   RT @SDAgriculture: Thank you @SDGovDaugaard fo...     C   \n",
       "844170   ICYMI: Always enjoy mornings with @cspan @cspa...     C   \n",
       "536961   Kathleen's Women- &amp; Minority-Owned Busines...     L   \n",
       "901461   Members of @VETSports discuss efforts to assis...     L   \n",
       "1189330  Bad #trade deals have resulted in lost #jobs a...     L   \n",
       "333424   The Ag portion of the minibus really focuses o...     C   \n",
       "693819   What's in the bill? Critical support for:\\n\\n•...     L   \n",
       "605614   Looking forward to joining @CNNSOTU on Sunday ...     L   \n",
       "1217669  #Bismarck Century students Ronak, Bryce, Erik ...     C   \n",
       "222200   Today is #TaxDay, which is a good reminder tha...     L   \n",
       "1302774            Read more here: https://t.co/M4qq2JBpGi     C   \n",
       "1252904  Thank you to @Sandridge172 in #Lywood for the ...     L   \n",
       "745510   Larry Willis - President, Transportation Trade...     L   \n",
       "443974   This is not a trifling matter. I urge my colle...     C   \n",
       "1263565  RT @LasVegas_NV_USA: Titus looking out for ani...     L   \n",
       "626827   RT @75arcounties: Thank you @RepFrenchHill for...     C   \n",
       "809495   This is yet another shameful example of how @B...     L   \n",
       "291929   Joined my fellow Minnesotans in Washington, D....     C   \n",
       "1180205  .@txlegion plays such an important role in ser...     C   \n",
       "\n",
       "                                              clean_tweets  \n",
       "1098648  do not know like live paycheck paycheck hell n...  \n",
       "460691   thank declare yesterday e15 day south dakota -...  \n",
       "844170            icymi always enjoy morning thank cspanwj  \n",
       "536961   kathleens women minorityowned business resourc...  \n",
       "901461   member discuss effort assist veteran veteransd...  \n",
       "1189330  bad trade deal result lose job shuttered facto...  \n",
       "333424   the ag portion minibus really focus ruralameri...  \n",
       "693819   what s bill critical support forpuerto ricos r...  \n",
       "605614    look forward join sunday morning tune 8 a.m. cst  \n",
       "1217669  bismarck century student ronak bryce erik doug...  \n",
       "222200   today taxday good reminder goptaxscam ultimate...  \n",
       "1302774                                               read  \n",
       "1252904  thank lywood warm welcome yesterday -PRON- ple...  \n",
       "745510   larry willis president transportation trades d...  \n",
       "443974   this trifle matter -PRON- urge colleague congr...  \n",
       "1263565                                  titus look animal  \n",
       "626827   thank speak county judge morning spring meetin...  \n",
       "809495   this yet another shameful example care way mak...  \n",
       "291929   joined fellow minnesotans washington dc evenin...  \n",
       "1180205  play important role serve community teaching k...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @aafb: Congrats to ⁦@RepOHalleran⁩ &amp;amp; ⁦@...</td>\n",
       "      <td>L</td>\n",
       "      <td>congrat appointment look forward work together h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet class  \\\n",
       "0  RT @aafb: Congrats to ⁦@RepOHalleran⁩ &amp; ⁦@...     L   \n",
       "\n",
       "                                       clean_tweets  \n",
       "0  congrat appointment look forward work together h  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('outdata/tweets_clean_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    \n",
    "s3.meta.client.upload_file('outdata/tweets_clean_df.pkl',\n",
    "                           bucket_name,\n",
    "                           'tweets_clean_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('outdata/tweets_clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
